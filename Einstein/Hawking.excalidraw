{
  "type": "excalidraw",
  "version": 2,
  "source": "https://marketplace.visualstudio.com/items?itemName=pomdtr.excalidraw-editor",
  "elements": [
    {
      "id": "fAZO-GKJgLCEkt71doKp6",
      "type": "text",
      "x": 178.38671875,
      "y": -112.7578125,
      "width": 1788.2188720703125,
      "height": 5075,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a0",
      "roundness": null,
      "seed": 1680323367,
      "version": 92,
      "versionNonce": 1578901513,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1768641095669,
      "link": null,
      "locked": false,
      "text": "### 1️⃣ Problem definition (1–2 minutes)\n\n* Separate backend services per Salesforce object (e.g., Lead, Opportunity, Account), and each service integrated with Salesforce differently\n* Integration was tight and synchronous, so failures cascaded and debugging was painful\n- Burned engineering time without improving quality or user value\n* Maintenance cost was high because there was no common platform or shared integration pattern across services\n* Training implemented differently per object, which led to inconsistent user experience and uneven model quality\n\n**Goal**\n\n> Build a single, stable integration surface for Salesforce and move heavy compute to an async, tenant-isolated ML platform with reliable training + prediction and clear model lifecycle.\n\n---\n\n### 2️⃣ Functional requirements (3–4 minutes)\n\n* Tenant onboarding (create tenant namespace + metadata)\n* Consent management\n\n  * allowTraining (use data for training)\n  * allowStorePredictions (persist outcomes)\n* Bulk export from Salesforce (per tenant) into staging (S3/lakehouse)\n* Training orchestration per tenant (start, retry, resume, cancel)\n* Store artifacts + metrics, register model versions, promote active model\n* On-demand prediction for a Salesforce record (lead/opportunity/account etc.)\n* Support both sync and async prediction modes\n* Write results back into Salesforce objects/fields and/or Salesforce-hosted store\n* Observability and admin APIs for status, run history, and failures\n\n---\n\n### 3️⃣ Non-functional requirements (2–3 minutes)\n\nHighlight only what drives architecture:\n\n* High availability for prediction path (99.9% target)\n* Eventual consistency acceptable for training and async prediction callbacks\n* Multi-tenant isolation (data + model + access)\n* Burst handling (spiky predictions, bulk exports)\n* Idempotency and resumability (exports, training runs, prediction requests)\n* Compliance: consent gating must be enforced server-side\n* Encryption in transit + at rest, least privilege IAM\n\nThis justifies **single integration layer + event-driven queues + orchestrated ML pipeline**.\n\n---\n\n### 4️⃣ APIs (2 minutes)\n\nKeep it short and interview-ready:\n\n**Tenant + consent**\n\n* `POST /v1/tenants`\n* `POST /v1/tenants/{tenantId}/consent`\n* `GET  /v1/tenants/{tenantId}/status`\n\n**Export + training**\n\n* `POST /v1/tenants/{tenantId}/exports`\n* `GET  /v1/tenants/{tenantId}/exports/{exportJobId}`\n* `POST /v1/tenants/{tenantId}/training-runs`\n* `GET  /v1/tenants/{tenantId}/training-runs/{trainingRunId}`\n\n**Prediction**\n\n* Sync:  `POST /v1/tenants/{tenantId}/predict`\n* Async: `POST /v1/tenants/{tenantId}/predict:async` + `GET /v1/predictions/{requestId}`\n\n**Callbacks to Salesforce (internal integration)**\n\n* `POST /v1/salesforce/callbacks/prediction-complete`\n* `POST /v1/salesforce/callbacks/model-ready`\n\nMention **direct multi-service SF integration only to reject it**. Shows judgment.\n\n---\n\n### 5️⃣ Data model + partitioning (5 minutes)\n\nExplain the minimum entities that make the system real:\n\n* **Tenant**: `tenant_id`, `sf_org_id`, `status`, timestamps\n* **Consent**: `tenant_id`, `allow_training`, `allow_store_predictions`, `updated_by`, `version`\n* **ExportJob**: `export_job_id`, `tenant_id`, `sf_bulk_job_id`, `dataset_uri`, `schema_hash`, `status`\n* **TrainingRun**: `training_run_id`, `tenant_id`, `export_job_id`, `status`, `sagemaker_job_arn`, `metrics_uri`, `model_artifact_uri`\n* **ModelRegistry**: `tenant_id`, `model_version`, `status (staged/active/deprecated)`, `artifact_uri`\n* **PredictionRequest**: `request_id` (idempotency key), `tenant_id`, `entity_type`, `entity_id`, `status`, `model_version_used`, `result_uri`\n\nPartitioning / isolation (this is the key point):\n\n* **Primary partition key: `tenant_id`**\n* Storage layout: `s3://…/{tenant_id}/{export_job_id}/...` and `.../{model_version}/...`\n* DB indexes: `(tenant_id, status)`, `(tenant_id, created_at)`, `(tenant_id, entity_id)`\n\nThis naturally leads into HLD.\n\n---\n\n### 6️⃣ High-level design (5–7 minutes)\n\nYou already have the right architecture. Walk through **one clean training path** and **one clean prediction path**.\n\n**Training path (write-heavy, async)**\n\n* Salesforce (Einstein/app) → API Gateway\n* Tenant Service validates consent + config\n* Export Worker uses SF Bulk API → writes dataset to S3/lakehouse\n* Orchestrator creates TrainingRun → sends message to `SQS.training`\n* Workers/Lambda start SageMaker training job\n* Artifacts + metrics → S3\n* ModelRegistry updated → new active model (optional gating)\n* Callback worker updates Salesforce (model-ready + metadata)\n\n**Prediction path (read/compute, sync + async)**\n\n* Salesforce → API Gateway → Inference Service\n* Resolve tenant active model version\n* **Sync**: call warm model endpoint or cached runtime → return prediction\n* **Async**: enqueue `SQS.inference` → worker runs prediction → callback writes back to Salesforce\n\nDon’t enumerate every box. Focus on flow and why boundaries exist.\n\n---\n\n### 7️⃣ Deep dive (10–15 minutes)\n\nSlow down here. Pick **1–2 problems** that show engineering depth.\n\n#### Deep dive A: Consent-gated training correctness under retries (the “hard” one)\n\nCover only:\n\n* **Server-side consent gate**\n\n  * Orchestrator checks `Consent.allow_training == true` at training-run creation time\n  * Option: re-check right before starting SageMaker job (protect against mid-flight revoke)\n* **Idempotency**\n\n  * `trainingRunId` is the idempotency key\n  * Workers must be able to receive duplicates and do nothing if already `Succeeded/Running`\n* **At-least-once delivery**\n\n  * SQS may deliver duplicate messages\n  * Worker logic: read TrainingRun state → only start job if state == `Queued`\n* **Resumability**\n\n  * ExportJob and TrainingRun statuses are persisted\n  * If export fails, retry export without duplicating datasets (use `exportJobId` path)\n* **DLQ + replay**\n\n  * Poison messages go to DLQ with reason\n  * Operator can replay after fixing config/schema mismatch\n\nThis is where interviewers engage because it’s real failure-mode thinking.\n\n#### Deep dive B: Prediction spike control and callback reliability\n\nCover only:\n\n* **Sync vs async switching**\n\n  * Use API Gateway throttles per tenant\n  * If queue depth high or endpoint saturated → force async\n\n* **Model loading strategy**\n\n  * Warm endpoints for top tenants (latency)\n  * Lambda runtime load for long tail (cost), accept cold-start\n  * Hybrid: keep N hottest models warm\n\n* **Request idempotency**\n\n  * `requestId` required from Salesforce\n  * if same requestId repeats → return stored result or current status\n\n* **Callback delivery**\n\n  * Salesforce write-back can fail (rate limits, transient errors)\n  * Use retry with backoff + “pending callback” status\n  * Persist last error + next retry time for ops visibility\n\n---\n\n### 8️⃣ Tradeoffs and closing (2 minutes)\n\nEnd strong:\n\n* Async pipeline adds backend complexity (orchestrator, queues, retries)\n* Eventual consistency for training and async prediction callbacks\n* Some latency/cost tradeoffs depending on warm endpoints vs Lambda model loads\n* But you get:\n\n  * Cleaner Salesforce integration (single surface)\n  * Much better reliability under spikes\n  * Tenant isolation + consent enforcement\n  * Real model lifecycle and auditability\n  * Easier evolution over time (new models/features without breaking SF)\n\n---\n\nIf you want, I can also compress this into a 7–8 minute “spoken script” version with exactly what to say per section, plus 5–6 crisp diagrams you can redraw on a virtual whiteboard.\n",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "### 1️⃣ Problem definition (1–2 minutes)\n\n* Separate backend services per Salesforce object (e.g., Lead, Opportunity, Account), and each service integrated with Salesforce differently\n* Integration was tight and synchronous, so failures cascaded and debugging was painful\n- Burned engineering time without improving quality or user value\n* Maintenance cost was high because there was no common platform or shared integration pattern across services\n* Training implemented differently per object, which led to inconsistent user experience and uneven model quality\n\n**Goal**\n\n> Build a single, stable integration surface for Salesforce and move heavy compute to an async, tenant-isolated ML platform with reliable training + prediction and clear model lifecycle.\n\n---\n\n### 2️⃣ Functional requirements (3–4 minutes)\n\n* Tenant onboarding (create tenant namespace + metadata)\n* Consent management\n\n  * allowTraining (use data for training)\n  * allowStorePredictions (persist outcomes)\n* Bulk export from Salesforce (per tenant) into staging (S3/lakehouse)\n* Training orchestration per tenant (start, retry, resume, cancel)\n* Store artifacts + metrics, register model versions, promote active model\n* On-demand prediction for a Salesforce record (lead/opportunity/account etc.)\n* Support both sync and async prediction modes\n* Write results back into Salesforce objects/fields and/or Salesforce-hosted store\n* Observability and admin APIs for status, run history, and failures\n\n---\n\n### 3️⃣ Non-functional requirements (2–3 minutes)\n\nHighlight only what drives architecture:\n\n* High availability for prediction path (99.9% target)\n* Eventual consistency acceptable for training and async prediction callbacks\n* Multi-tenant isolation (data + model + access)\n* Burst handling (spiky predictions, bulk exports)\n* Idempotency and resumability (exports, training runs, prediction requests)\n* Compliance: consent gating must be enforced server-side\n* Encryption in transit + at rest, least privilege IAM\n\nThis justifies **single integration layer + event-driven queues + orchestrated ML pipeline**.\n\n---\n\n### 4️⃣ APIs (2 minutes)\n\nKeep it short and interview-ready:\n\n**Tenant + consent**\n\n* `POST /v1/tenants`\n* `POST /v1/tenants/{tenantId}/consent`\n* `GET  /v1/tenants/{tenantId}/status`\n\n**Export + training**\n\n* `POST /v1/tenants/{tenantId}/exports`\n* `GET  /v1/tenants/{tenantId}/exports/{exportJobId}`\n* `POST /v1/tenants/{tenantId}/training-runs`\n* `GET  /v1/tenants/{tenantId}/training-runs/{trainingRunId}`\n\n**Prediction**\n\n* Sync:  `POST /v1/tenants/{tenantId}/predict`\n* Async: `POST /v1/tenants/{tenantId}/predict:async` + `GET /v1/predictions/{requestId}`\n\n**Callbacks to Salesforce (internal integration)**\n\n* `POST /v1/salesforce/callbacks/prediction-complete`\n* `POST /v1/salesforce/callbacks/model-ready`\n\nMention **direct multi-service SF integration only to reject it**. Shows judgment.\n\n---\n\n### 5️⃣ Data model + partitioning (5 minutes)\n\nExplain the minimum entities that make the system real:\n\n* **Tenant**: `tenant_id`, `sf_org_id`, `status`, timestamps\n* **Consent**: `tenant_id`, `allow_training`, `allow_store_predictions`, `updated_by`, `version`\n* **ExportJob**: `export_job_id`, `tenant_id`, `sf_bulk_job_id`, `dataset_uri`, `schema_hash`, `status`\n* **TrainingRun**: `training_run_id`, `tenant_id`, `export_job_id`, `status`, `sagemaker_job_arn`, `metrics_uri`, `model_artifact_uri`\n* **ModelRegistry**: `tenant_id`, `model_version`, `status (staged/active/deprecated)`, `artifact_uri`\n* **PredictionRequest**: `request_id` (idempotency key), `tenant_id`, `entity_type`, `entity_id`, `status`, `model_version_used`, `result_uri`\n\nPartitioning / isolation (this is the key point):\n\n* **Primary partition key: `tenant_id`**\n* Storage layout: `s3://…/{tenant_id}/{export_job_id}/...` and `.../{model_version}/...`\n* DB indexes: `(tenant_id, status)`, `(tenant_id, created_at)`, `(tenant_id, entity_id)`\n\nThis naturally leads into HLD.\n\n---\n\n### 6️⃣ High-level design (5–7 minutes)\n\nYou already have the right architecture. Walk through **one clean training path** and **one clean prediction path**.\n\n**Training path (write-heavy, async)**\n\n* Salesforce (Einstein/app) → API Gateway\n* Tenant Service validates consent + config\n* Export Worker uses SF Bulk API → writes dataset to S3/lakehouse\n* Orchestrator creates TrainingRun → sends message to `SQS.training`\n* Workers/Lambda start SageMaker training job\n* Artifacts + metrics → S3\n* ModelRegistry updated → new active model (optional gating)\n* Callback worker updates Salesforce (model-ready + metadata)\n\n**Prediction path (read/compute, sync + async)**\n\n* Salesforce → API Gateway → Inference Service\n* Resolve tenant active model version\n* **Sync**: call warm model endpoint or cached runtime → return prediction\n* **Async**: enqueue `SQS.inference` → worker runs prediction → callback writes back to Salesforce\n\nDon’t enumerate every box. Focus on flow and why boundaries exist.\n\n---\n\n### 7️⃣ Deep dive (10–15 minutes)\n\nSlow down here. Pick **1–2 problems** that show engineering depth.\n\n#### Deep dive A: Consent-gated training correctness under retries (the “hard” one)\n\nCover only:\n\n* **Server-side consent gate**\n\n  * Orchestrator checks `Consent.allow_training == true` at training-run creation time\n  * Option: re-check right before starting SageMaker job (protect against mid-flight revoke)\n* **Idempotency**\n\n  * `trainingRunId` is the idempotency key\n  * Workers must be able to receive duplicates and do nothing if already `Succeeded/Running`\n* **At-least-once delivery**\n\n  * SQS may deliver duplicate messages\n  * Worker logic: read TrainingRun state → only start job if state == `Queued`\n* **Resumability**\n\n  * ExportJob and TrainingRun statuses are persisted\n  * If export fails, retry export without duplicating datasets (use `exportJobId` path)\n* **DLQ + replay**\n\n  * Poison messages go to DLQ with reason\n  * Operator can replay after fixing config/schema mismatch\n\nThis is where interviewers engage because it’s real failure-mode thinking.\n\n#### Deep dive B: Prediction spike control and callback reliability\n\nCover only:\n\n* **Sync vs async switching**\n\n  * Use API Gateway throttles per tenant\n  * If queue depth high or endpoint saturated → force async\n\n* **Model loading strategy**\n\n  * Warm endpoints for top tenants (latency)\n  * Lambda runtime load for long tail (cost), accept cold-start\n  * Hybrid: keep N hottest models warm\n\n* **Request idempotency**\n\n  * `requestId` required from Salesforce\n  * if same requestId repeats → return stored result or current status\n\n* **Callback delivery**\n\n  * Salesforce write-back can fail (rate limits, transient errors)\n  * Use retry with backoff + “pending callback” status\n  * Persist last error + next retry time for ops visibility\n\n---\n\n### 8️⃣ Tradeoffs and closing (2 minutes)\n\nEnd strong:\n\n* Async pipeline adds backend complexity (orchestrator, queues, retries)\n* Eventual consistency for training and async prediction callbacks\n* Some latency/cost tradeoffs depending on warm endpoints vs Lambda model loads\n* But you get:\n\n  * Cleaner Salesforce integration (single surface)\n  * Much better reliability under spikes\n  * Tenant isolation + consent enforcement\n  * Real model lifecycle and auditability\n  * Easier evolution over time (new models/features without breaking SF)\n\n---\n\nIf you want, I can also compress this into a 7–8 minute “spoken script” version with exactly what to say per section, plus 5–6 crisp diagrams you can redraw on a virtual whiteboard.\n",
      "autoResize": true,
      "lineHeight": 1.25
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff"
  },
  "files": {}
}